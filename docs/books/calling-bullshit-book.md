---
UUID: 20230204080817
Created: '2023-02-04 08:08'
Modified: '2025-03-28 17:12'
tags: []
Author:
  - '[[Carl T. Bergstrom]]'
  - '[[Jevin D. West]]'
Genre: Political Science
Stored: Kindle
ReadingStatus: Read
excalidraw-plugin: parsed
excalidraw-autoexport: png
excalidraw-open-md: true
Purchased: true
Fiction: false
Version: 1.02
Pages: 336
Rate: 5
PublishDate: 2020-08-04T00:00:00.000Z
FinishDate: 2023-02-04T00:00:00.000Z
Cover: >-
  http://books.google.com/books/content?id=Plu9DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api
URL: 'https://www.goodreads.com/book/show/48889983-calling-bullshit'
draft: false
SiteProcssed: true
---

# Calling Bullshit (book)

## ✒️ Note-Making

### 🔗Connect

⬆️Topic:: [Communication (MOC)](/mocs/communication-moc.md)
⬆️Topic:: [Statistics (MOC)](/mocs/statistics-moc.md)

### 💡Clarify
🔈 *Summary of main ideas*
1. **Bullshit is everywhere** - Due to the rise of the attention economy, and the decentralization of communication, the amount of unverified data has exploded in recent years, which reduces the chances for verification while increasing the incentives for misleading data.
2. **Statistics for all** - One of the best ways to identify bullshit is to be familiar with statistical concepts. Things like selection bias, garbage in garbage out, correlation vs causation, and misrepresentation of data are all examples of ways data can mislead us, either intentional or unintentional. If it's too good to be true, or no source included, it's probably false.
3. **Combating bullshit** - it's hard to refute bullshit because it is easier to generate it than to prove it false. While it is a difficult task, we have to approach it none the less. If it is on social media, try to point out it's flaws. If it is a more personal setting, be humble, compassionate, and remember that you might be wrong too.

### 🗒️Relate

⛓ *Life lessons, action items*

### 🔍Critique

✅ *by following this method, what will happen?*
This book is a must for any content consumer in our digital age, which is basically everyone. The book is a perfect combination between the general theory, what to look for, how to debunk it, and how to do so in a human matter.

Even as a data analyst, who deals with messages, visualization, and hypothesis testing in my daily life, I still managed to learn useful things while reading this book, so i'm sure its going to be a gold-mind for others as well.

❌ *the logical jumps, holes or simply cases where it is wrong...*

🧱 *Implementations and limitations of it are...*

### 🗨️Review

💭 *my opinions on the book, the writers style...*

### 🖼️Outline

![Calling Bullshit (book).webp](/books/calling-bullshit-book.webp)
## 📒 Notes

### Intro

Bullshit is everywhere, and it has evolved to using math and statistic to rob us of ways to combat it, since in order to object to bullshit, you need:
1. [Critical Thinking](/notes/critical-thinking.md) - usually stems from humanities - philosophy, law, politics
2. **statistical knowledge** - usually comes from STEM subjects.

**since most people don't have both, we are left defenseless from others who are trying to trick us**

:::note[PREFACE]

- New-school bullshit uses the language of math and science and statistics to create the impression of rigor and accuracy.
- New-school bullshit can be particularly effective because many of us don’t feel qualified to challenge information that is presented in quantitative form.

:::


### Chapter 1 - Bullshit is Everywhere

lying has a strong negative implication, you might be hurt, outcasted, or lose your trust if caught lying.
**Bullshiting, however, is a form of using the gap between the literal meaning of words, and the context or implication of words to mislead or hide the truth** [Semantics of words](/notes/semantics-of-words.md). In those cases, you can also fall back to saying that technically what you said is true, so you shouldn't be blamed.
for example: saying "john doesn't use heroine at work" is something that literally is true, and john wouldn't want to claim otherwise, but the [Context](/notes/context.md) is implying that john is a heroine user in general.
Also, to be an effective bullshiter, you must have a [Theory of Mind](/notes/theory-of-mind.md), understand how others view the world, what they care about, and how can they be triggered.
**Communication is therefore a two way sword. It enables great feats of cooperation, but also deception.**

the main problems with bullshit are:
1. its harder to clean than to produce
2. it speared faster than you can combat it
3. it takes less intelligence to produce it

:::note[Bullshit Everywhere]

- A sophisticated bullshitter needs a theory of mind—she needs to be able to put herself in the place of her mark. She needs to be able to think about what the others around her do and do not know. She needs to be able to imagine what impression will be created by what sort of bullshit, and to choose her bullshit accordingly.
- By communicating we can work together in remarkable ways. But by paying attention to communication, you are giving other people a “handle” they can use to manipulate your behavior.
- An important genre of bullshit known as weasel wording uses the gap between literal meaning and implicature to avoid taking responsibility for things.
- (1) bullshit takes less work to create than to clean up, (2) takes less intelligence to create than to clean up, and (3) spreads faster than efforts to clean it up.

:::


### Chapter 2 - "the Rise of bullshit"

dangers of misconception has always existed, even during the rise of the print press people feared the spread of misinformation, but the internet has magnified that fear, by:
1. **complete democratization of distributing information** - anyone can be a publisher of information, with complete access to everyone in the world, without having to go through any sort of education/regulation. [decentralization](/notes/decentralization.md)
2. **Advertising incentives are stronger than truth** - in the past, people paid subscriptions to news medias, which created a dependency of accuracy and trust to keep that connection alive. Today sites are driven by advertisements and not subscriptions, which changes the incentives to make you click, now, whatever the cause may be, without any thoughts about long term, so no considerations of accuracy. Headlines today are trying to make you feel something, instead of knowing something. [Incentives](/notes/incentives.md) [Attention Economy](/notes/attention-economy.md)
3. **Creation of eco chambers** - media outlets have been sorted into eco chambers based on political views, which increase the gap and intensity of those beliefs, and decreases the levels of stable communication between groups. [us vs them](/notes/us-vs-them.md)
4. **social acceptance** - activity on social media is more an act of belonging to a group, rather than delivering information. Therefore we will tend to present stories that match our beliefs, rather than criticize misinformation shared by our peers. [Herd Mentality](/notes/social-environment.md)

:::note[Medium, Message, and Misinformation]

- The Internet has changed the way we produce, share, and consume information. It has altered the way we do research, learn about current events, interact with our peers, entertain ourselves, and even think.
- The Internet site is not necessarily designed to perpetuate a long-term relationship; it is designed to make you click, now. Quality of information and accuracy are no longer as important as sparkle.
- the most successful headlines don’t convey facts, they promise you an emotional experience.
- Participating on social media is only secondarily about sharing new information; it is primarily about maintaining and reinforcing common bonds.
- Disinformation flows through a network of trusted contacts instead of being injected from outside into a skeptical society.
- “The point of modern propaganda isn’t only to misinform or push an agenda. It is to exhaust your critical thinking, to annihilate truth.”

:::


### Chapter 3 - what is Bullshit

A particular aspect of bullshit is the "black box", more relevant in cases where bullshitting involves data. The mechanism that generates the bullshit might sometimes be so complicated that its hard to refute. However, **in many cases we can identify the bullshit by digging dip to the sources of the data**. As the saying goes - [Garbage in garbage out](/notes/garbage-in-garbage-out.md), **sometimes bullshit is nothing more than an honest attempt to convey information that is built on wrong assumptions / its data includes biases**.
for example, a research that tries to identify criminals, was based on some metrics involved in smiling. However, it appears that the research was founded on personal photos of non criminal, and ID photos of criminal, where they are more likely to not smile. This error in the training data can lead to bad results in the model.

:::note[The Nature of Bullshit]

- Bullshit involves language, statistical figures, data graphics, and other forms of presentation intended to persuade or impress an audience by distracting, overwhelming, or intimidating them with a blatant disregard for truth, logical coherence, or what information is actually being conveyed.
- One can obtain stupid results from bad data without any statistical trickery. And this is often how bullshit arguments are created, deliberately or otherwise.
- A machine learning algorithm is only as good as its training data,

:::


### Chapter 4 - Causation

[Correlation is not causation](/notes/correlation-is-not-causation.md), and yet we are too quickly assuming a connection between the two. When I see that A and B are correlated, I can't know for certain if:
1. A -> B
2. B -> A
3. C -> A & B
4. random noise
5. any other complicated mechanism

:::note[Causality]

- correlation does not imply causation. Do not leap carelessly from data showing the former to assumptions about the latter.
- There is a key distinction between a probabilistic cause (A increases the chance of B in a causal manner), a sufficient cause (if A happens, B always happens), and a necessary cause (unless A happens, B can’t happen).

:::


### Chapter 5 - Numbers

we tend to see numbers as absolute truths, but even if we are presented with the correct numbers, they can mislead us. **Numbers must come with context, and allow for the correct comparison**.

when numbers are misleading
1. **the data is based on a biased input** (using only basketball players to measure population height)
2. **Aggregate measures hide variance** - using average effect usually hides the extreme effects on both ends (like a tax cut for the rich that "on average" will save each family x dollars)
3. **numbers without a comparison group** - x people who did y have committed crimes. what is the ratio of that group? what is a ratio for people who didn't do y?
4. **percentage vs percentage points** - change is more extreme in small numbers, also percentages sometimes hide the "true story" within the denominator, or complications with negative numbers
5. **Goodharts law** - if its a metric, it has probably became a bad measure since people are trying to trick the system. [Goodhart’s Law](/notes/goodhart’s-law.md)
6. **"mathiness"** - equations that look like math (trust * fun / arguments = love), but are essentially meaningless. Its not clear why they chose this type of formula over others. Also - how do you quantify those things? what is the unit of measure? and how do they interact?
7. **zombie statistics** - outdated and possibly false statistics with no source that keeps on getting cited

:::note[Numbers and Nonsense]

- We are told that “the data never lie.” But this perspective can be dangerous. Even if a figure or measurement is correct, it can still be used to bullshit,
- For numbers to be transparent, they must be placed in an appropriate context. Numbers must presented in a way that allows for fair comparisons.
- if sufficient rewards are attached to some measure, people will find ways to increase their scores one way or another, and in doing so will undercut the value of the measure for assessing what it was originally designed to assess.
- Mathiness refers to formulas and expressions that may look and feel like math—even as they disregard the logical coherence and formal rigor of actual mathematics.
- Zombie statistics are numbers that are cited badly out of context, are sorely outdated, or were entirely made up in the first place

:::


### Chapter 6 - Selection Bias

[Selection Bias](/notes/selection-bias.md) is an essential problem in statistics. **Since we can never survey the entire population, we have to rely on samples that represent the general population**, however selection bias can often happen when:
1. **The sample isn't a random representation** of society - asking only people in a gun convention what are their opinions on gun control
2. **The sampling itself might add bias** - like asking people how often they cheat on tests, they will give a dishonest answer
3. **The sampling processed is biased** - surveying people by using smartphones will necessarily lead to mis-representation of communities that don't use smartphones. or [Survivors Bias](/notes/survivors-bias.md), checking only planes that returned to see where they are damaged.

:::note[Selection Bias]

- what you see depends on where you look. To draw valid conclusions, we have to be careful to ensure that the group we look at is a random sample of the population.
- We also need to think about whether the results we get are influenced by the act of sampling itself.
- selection bias. Selection bias arises when the individuals that you sample for your study differ systematically from the population of individuals eligible for your study.

:::


#### Other Phenomenon
"right censorship"
**right-censourship causes a type of selection bias by filtering out cases of a particular sub-group**. For example, estimating life expectancy by generations. If you filter out all those who are still alive will leave you with only those who died prematurely, showing an alarming low level of life expectancy for younger generations.

observation bias
"why are all the hot guys (i'm dating) are jerks", it seems that there is a negative correlation between niceness and hotness, but is it truly the case?
**this is actually an error of observing only a subset of the entire population, in a way that generates a misleading correlation.**
meaning - on the general population, there is no connection between the two,
however, when you filter our people who you will never date, and people who will never date you, you are left with a narrow part of population that is neither perfect nor "the worst", so a certain trade off between qualities is expected. its called the [Berkson's paradox](/notes/berksons-paradox.md).
This trade-off means that you get a fake correlation between the two (look at the following blue section within the chart, which is negatively correlated)

![Berkson's paradox.webp](/notes/berksons-paradox.webp)

### Chapter 7 - Data Visualization

in general, **good visualizations maintain the "law of proportional ink", which means that the size of the visualization matches the actual value of the data.**
we should be aware of:
1. **"ducks"** - visualizations with a theme that is meant to distract us
2. **axis manipulation**:
	1. bar charts that don't start with 0
	2. line charts that are too zoomed out/in that it messes with the trend
	3. distribution bins with unequal sizes
5. **unweighted data** - for examples to show accidents per age, without providing the mileage for example per age group, or number of drivers, etc...

:::note[Data Visualization]

- Our educational system has not caught up. Readers may have little training in how to interpret data graphics.
- Ducks are like clickbait for the mind; instead of generating a mouse click, they are trying to capture a few seconds of your attention. Whereas a bar graph or line chart may seem dry and perhaps complicated, a colorful illustration may seem fun enough and eye-catching enough to draw you in.
- Always look at the axes when you see a data graphic that includes them.
- we need to be on the lookout for uneven or varying scales on the x axis.

:::


### Chapter 8 - Machine Learning

[Machine Learning](/notes/machine-learning.md)
common problems with the results of an ML model:
1. **GIGO (garbage in, garbage out)** - the model is only as good as it's training data.
2. **Overfitting** - complicated models that learn "too much" would treat random noise as relevant, which will make them worse on the test data. [Overfitting](/notes/overfitting.md)
3. **Dimensionality** - the more features you use, the likelier that a combination will have a correlation by random change. Therefore you need increasingly more data.
4. **Bias inheritance** - the model will try to make sense of all that its given. If the original data includes a bias, the model could treat it as relevant. such as racial profiling. Similarly, the model could use features we didn't thought would be relevant, like looking at the background of the photo when analyzing pictures. [AI alignment problem](/notes/ai-ethics.md)

:::note[Calling Bullshit on Big Data]

- Begin with bad data and labels, and you’ll get a bad program that makes bad predictions in return. This happens so often that there is an acronym for it in computer science: GIGO—garbage in, garbage out.
- Much of the time, the only limit to the computer’s learning capacity is the availability of high-quality labels with which to train the machine.
- overfitting—classifying noise as relevant information when making predictions. Overfitting is the bane of machine learning.
- Complicated models do a great job of fitting the training data, but simpler models often perform better on the test data than more complicated models.
- No algorithm, no matter how logically sound, can overcome flawed training data.
- To call bullshit on the newest AI startup, it is often sufficient to ask for details about the training data. Where did it come from? Who labeled it? How representative is the data?
- Machines are not free of human biases; they perpetuate them, depending on the data they’re fed.
- The more variables you add, the more training data you need.
- If you add enough variables into your black box, you will eventually find a combination of variables that performs well—but it may do so by chance.

:::


### Chapter 9 - Science Validity

We often tend to confuse [conditional probability](/notes/conditional-probability.md), we focus too much on the chance of an event happening, rather than on the chance that that event is a true positive.
For example, a chance of having a false finger print match is only 1 in 10 millions, that sounds good. But given that the entire dataset is 50 million rows, the chance of a true positive, given a match, is only 1/6.
so for comparison:
P(innocent | match) = 1/6, vs P(match | Innocent) = 1/10000000.

How can we be certain the a result of a study is correct?
the main way of doing that is by using (Related: [P Value](/notes/p-value.md)).
In a perfect world, we would have the ability to deductively find the probability of the existence of the phenomenon that we are testing. However, **science works by induction, not deduction. Everything is plausible until proven false.**
The [scientific method](/notes/scientific-method.md) works by using [Hypothesis Testing](/notes/hypothesis-testing.md), which is setting up a null hypothesis - the phenomenon doesn't exist, and if the results are good enough we can reject that hypothesis. But we can never deduct that the alternative is true.
P-values shows us what is the probability of us getting these extreme results by chance. Extreme = the difference our phenomenon makes, for example the difference in treatment days in comparing those who received the cure and those who didn't.
So a p-value of 5% means that on 5% of the cases we will get a *false positive*.

Some researches tend to do *P-hacking* in order to publish their findings and improve their status.
You can be p-hack by Adjusting your data, selecting specific samples in order to generate the phenomenon you want to see. For example choosing only the healthiest patients who got the cure, vs the worst from those who didn't.

all in all, a scientific paper might be misleading due to:
1. **p-hacking** - since scientists won't publish a negative result
2. **click-baiting** - news outlets converting scientific proof into click-bait
3. **random chance** - you can always get false positives (5% p-value means that 5% of the results are false positives), so out of 100 researches, 5 of them will be false simply by chance.

:::note[The Susceptibility of Science]

- it would be a mistake to conclude that science, as practiced today, provides an unerring conduit to the heart of ultimate reality. Rather, science is a haphazard collection of institutions, norms, customs, and traditions that have developed by trial and error over the past several centuries.
- One of the reasons that science works so well is that it is self-correcting. Every claim is open to challenge and every fact or model can be overturned in the face of evidence.
- Scientists are stuck using p-values because they don’t have a good way to calculate the probability of the alternative hypothesis.
- Researchers sometimes try different statistical assumptions or tests until they find a way to nudge their p-values across that critical p = 0.05 threshold of statistical significance. This is known as p-hacking,
- Science writers sometimes engage in an analogous practice that we call cafeteria science. They pick and choose from a broad menu of studies, to extract a subset that tells a consistent and compelling story.
- No matter where a paper is published, no matter who wrote it, no matter how well supported its arguments may be, any paper can be wrong. Every hypothesis, every set of data, every claim, and every conclusion is subject to reexamination in light of future evidence.
- Usually the best you can hope to do is to determine that a paper is legitimate. By legitimate, we mean that a paper is (1) written in good faith, (2) carried out using appropriate methodologies, and (3) taken seriously by the relevant scientific community.
- Individual papers may be wrong and individual studies misreported in the popular press, but the institution as a whole is strong.

:::


### Chapter 10 - Spotting Bullshit
1. **Check the source** - *who* is sending the message, *how* does he know the content of the message, and *what* is he trying to sell you.
2. **Unfair comparisons** - beware of unfair comparisons, like saying that half of all COVID hospitalizations are for immunized people. (While ignoring the fact that they make up over 90% of the population)
3. **flattering results** - if it's too good to be true, it is possible is
4. **extreme results** - using *Fermi estimations*, which are educated guesses by factors of 10 (1/10/100 percent or 1/10/100 million, etc) will enable us to get rough estimates enough to spot bullshit even if we are not fully knowledgeable about the subject.
5. **avoid** [Conformation Bias](/notes/conformation-bias.md)
6. **test different hypothesis** - what if the cause of the effect shown is completely different than what is mentioned

:::note[Spotting Bullshit]

- Some sales jobs get you to part with your hard-earned money. Other sales jobs convince you to believe something that you didn’t believe before, or to do something you wouldn’t have done otherwise. Everyone is trying to sell you something; it is just a matter of figuring out what.
- Avoid confirmation bias. Confirmation bias is the tendency to notice, believe, and share information that is consistent with our preexisting beliefs.

:::


### Chapter 11 - Calling out Bullshit

methods to use for calling out bullshit
1. **redactum ad obserdium** - show how ridiculous the argument could be if used in other context or continued
2. **Redraw charts** - to be more accurate
3. **find counter examples**
4. **find better analogies**

also, remember that calling out bullshit is not done in a vacuum, it is not a contest between computers, humans are involved so:
1. **be compassionate** - if it is a friend or colleague, don't humiliate in public. talk to him 1 on 1, try to understand him and explain where he is wrong [positive language](/notes/positive-language.md)
2. **be humble** - remember that either you or him might be wrong, so assume that the [Debate](/notes/debate.md) is not a "clear cut" case [Hanlon's Razor](/notes/hanlons-razor.md) [Humility](/notes/humility.md)
3. **be clear** - speak to the point, don't try to assert dominance by overflowing the opponent with information, be direct and keep it simple. [Simplicity](/notes/simplicity.md)

:::note[Refuting Bullshit]

- It is not enough to simply debunk a myth; you need to replace it with an alternative account.

:::


