---
UUID: 20230710191055
Created: '2023-07-10 19:10'
Modified: '2024-11-26 17:53'
tags: []
excalidraw-plugin: parsed
excalidraw-autoexport: png
excalidraw-open-md: true
Version: 1.01
aliases: []
draft: false
SiteProcssed: true
---

# Distillment

## Notes

Distilling means to separate the [Noise](/notes/noise.md) from the core essence of something. To be aware of the distinction between [form vs essence](/notes/form-vs-essence.md). By removing the noise, we almost always receive a [smaller](/notes/atomism.md) idea, which would also make it [easier](/notes/simplicity.md) to understand, and to [utilize](/notes/transferred-learning.md) it in other cases [medium independent](/notes/medium-independent.md).

To distill an idea involves the following steps:
1. **Identification** - Identify the core message / idea within the content
2. **isolation** - Isolate the idea from the rest of the text, meaning that you need to add/subtract parts of that idea in order to make it a standalone idea, without needing to refer to the original text in order to understand it. [less is more](/notes/addition-by-subtraction.md)
3. **Process** - Write the idea in your own words, make it more approachable to you. [Writing](/notes/writing.md)

Distillment requires often a combination of the [Executive Attention Network](/notes/executive-attention-network.md) and our [downtime brain](/notes/downtime-brain.md). The first allows us to understand the idea, to criticize it, while the second allows to make it more abstract, to see "the soul" of the idea.
## Visual

![Distillment.webp](/notes/distillment.webp)

## Overview
ðŸ”¼Topic:: [Note taking (MOC)](/mocs/note-taking-moc.md)
ðŸ”¼Topic:: [Learning (MOC)](/mocs/learning-moc.md)
â—€Origin::
ðŸ”—Link::

