---
UUID: 20230406090453
Created: '2023-04-06 09:04'
Modified: '2026-02-03 08:34'
tags: []
excalidraw-plugin: parsed
excalidraw-autoexport: png
excalidraw-export-dark: false
excalidraw-open-md: true
Version: 2
Image: '[[Utilitarianism.png]]'
ImageText: Calculator
Description: >-
  An ethical framework positing that moral actions are those that maximize
  pleasure and minimize pain for the greatest number, often evaluated through a
  mathematical calculation of expected utility.
aliases: []
draft: false
SiteProcssed: true
---

# Utilitarianism

## Notes

Utilitarianism is an ethical framework that is based on the assumption that **all humans share a common trait: we seek pleasure and avoid pain.** Therefore, the moral action is such that maximizes pleasure. Utilitarianism usually involves a mathematical representation of the expected [Utility](/notes/utility.md) for each actor involved in the decision, and using that you can calculate the total expected utility from each choice, and select the one that has the highest output.

The two main disadvantages of Utilitarianism is:
1. **pleasure monsters** - imagine a person that derives endless pleasure from stealing and killing, more than the victims suffer from his actions. Should we allow him to act that way simply because the total pleasure is higher? [Love what is good](/notes/love-what-is-good.md)
2. **Ignorance on who gets hurt** - what if there was the option to sacrifice a person for the benefit of all? should we do it? How about killing people at the hospital so that we could save 5 people with their organs per person? Utilitarianism is ignorant on who gets hurt from these actions, it could be people from a certain group that will always be the one to suffer because they are the minority (creating a majority tyranny), or sacrificing innocent people for the "benefit of the group". [Indifference](/notes/indifference.md)

In summary, **Utilitarianism could lead to recommendations that are simply not common sense and goes against our basic moral intuitions.**

Personally, there are a few more reasons why utilitarianism would be a worse moral theory compared to others such as [Deontology](/notes/deontology.md) and [Virtue theory](/notes/virtue-theory.md):
1. **In turns us into calculators** - There is something cold and heartless about being a utilitarian, as if a function can tell us everything about human morality. I fear it would turn us into [Econs](/notes/econs.md), people who are governed by their rational side without taking emotions into account [Morality is both rational and emotional](/notes/morality-is-both-rational-and-emotional.md).
2. **It's too simplistic** - maximizing pleasure and reducing pain sounds too close to [Hedonism](/notes/hedonism.md), as if morality can be reduced to such a shallow metric.
3. **Unpractical due to uncertainty** - even if a function that manages to compare everyone's pain and pleasure level such that an outcome could be generated (which is highly unlikely, we can't even measure our own happiness sometimes), it is even more impossible considering [Uncertainty](/notes/uncertainty.md). Since we are bad at [Prediction](/notes/prediction.md), following utilitarianism is almost as random as it gets, because we have no concrete way of knowing which choice would actually maximize pleasure and reduce overall pain.

## Visual

![Utilitarianism](/notes/utilitarianism.webp)

## Overview
ðŸ”¼Topic:: [Ethics (MOC)](/mocs/ethics-moc.md)
â—€Origin::
ðŸ”—Link::

