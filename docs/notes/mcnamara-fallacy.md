---
UUID: 20220716082406
Created: '2022-07-16 08:24'
Modified: '2025-04-07 11:05'
tags: []
excalidraw-plugin: parsed
excalidraw-autoexport: png
excalidraw-open-md: true
Version: 1.03
aliases:
  - what can't be measured isn't important
draft: false
SiteProcssed: true
---

# McNamara Fallacy

## Notes

**The flawed assumption that what can't be measured isn't important**.
Usually it's not that we actively and consciously dismiss all other metrics as not important, rather that we subconsciously treat only what's already measured as important. It is similar to [Availability Bias](/notes/availability-bias.md), what is already measured is easier to recall which gives it a false pretense of importance. Additionally, we detest [Ambiguity](/notes/ambiguity.md), so anything that can't be [simplified](/notes/simplicity.md) is too cognitive taxing, so we prefer to treat it as not important.

This can also easily lead to [Goodhartâ€™s Law](/notes/goodhartâ€™s-law.md) because laziness can push us towards finding an easy proxy to an abstract (yet important) goal, which we will treat as the goal itself, thus missing the actual target and perhaps even causing unwanted consequences [shallow copy](/notes/shallow-copy.md). This essentially means that we also fall into the [streetlight effect](/notes/streetlight-effect.md), where we cling to what's easy to measure versus what is actually important.

Sometimes we notice that we fall for this fallacy when it's clear that the [Survivors Bias](/notes/survivors-bias.md) is also relevant. When the metrics we use don't explain much in terms of success rates, i.e those who "score high" fail and those who "score low" succeed.

Usually this fallacy happens when we focus on numeric metrics since they are the easiest to use, which causes qualitative metrics to be discarded as not important [Quantifiable self](/notes/quantifiable-self.md). However, much of [expertise](/notes/mastery.md) is based on knowledge that's difficult to quantify or even describe, but that doesn't mean that it's not important. It gets worse if we perceive what's measured not only as the most important metrics, but also that other metrics doesn't exist, i.e that these metrics are a pure representation of the information.

A way to reduce the chances of falling for this fallacy is to [Multitrack](/notes/multitrack.md), to use a variety of metrics which means we won't settle for the first who come to mind, rather force us to create more metrics which would hopefully will be a better representation of all that's important to measure.

Second, we must find ways of incorporating qualitative measures. While they are hard to quantify and reduce to something easier to digest and compare, they often contain valuable [Feedback](/notes/feedback.md).
## Visual

![McNamara Fallacy.webp](/notes/mcnamara-fallacy.webp)

## Overview
ðŸ”¼Topic:: [Cognitive Bias (MOC)](/mocs/cognitive-bias-moc.md)
Origin:: [The Curiosity Chronicle by Sahil Bloom](/notes/the-curiosity-chronicle-by-sahil-bloom.md)
ðŸ”—Link::

